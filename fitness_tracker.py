# -*- coding: utf-8 -*-
"""fitness tracker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xVIkEiDEUoyqOjQnlFfR-smqLLrsnb4W
"""

# Import necessary libraries
import pandas as pd

# Load the dataset
file_path = r"/content/fitness_data_medium.csv"
df = pd.read_csv(file_path)

# Display basic information about the dataset
print("Dataset Info:")
print(df.info())

# Display first few rows of the dataset
print("\nDataset Preview:")
print(df.head())

# Display column names
print("\nColumn Names:", df.columns)

from sklearn.preprocessing import LabelEncoder

# Check for missing values
print("Missing Values Before Cleaning:\n", df.isnull().sum())

# Drop rows with missing values (if any)
df.dropna(inplace=True)

# Encode categorical variables
if 'gender' in df.columns:
    le = LabelEncoder()
    df['gender'] = le.fit_transform(df['gender'])  # Convert 'Male'/'Female' to 0/1

# Verify if 'workout_intensity' exists before processing
if 'workout_intensity' in df.columns:
    intensity_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}
    df['workout_intensity'] = df['workout_intensity'].map(intensity_mapping)
else:
    print("‚ö†Ô∏è Warning: 'workout_intensity' column is missing in the dataset!")

# Check if 'stress' column exists before encoding
if 'stress_level' in df.columns:
    stress_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}
    df['stress_level'] = df['stress_level'].map(stress_mapping)
else:
    print("‚ö†Ô∏è Warning: 'stress' column is missing in the dataset!")

# Display processed dataset info
print("\nProcessed Dataset Info:")
print(df.info())

# Display first few rows after preprocessing
print("\nDataset Preview After Preprocessing:")
print(df.head())

print(df.dtypes)

# Convert user_id to numeric by removing 'U' and converting to integer
df["user_id"] = df["user_id"].str.replace("U", "").astype(int)
# Convert activity_level and fitness_goal to numeric values
df["activity_level"] = df["activity_level"].map({"Low": 1, "Moderate": 2, "High": 3})
df["fitness_goal"] = df["fitness_goal"].map({"Weight Loss": 1, "Muscle Gain": 2, "Endurance": 3, "General Fitness": 4})

# Now check data types again
print(df.dtypes)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define Features (X) and Targets (y1 for VO2_max, y2 for progress_score)
features = ["age", "gender", "bmi", "steps_count", "heart_rate_avg", "calories_burned", "stress_level", "hydration_level", "workout_intensity"]
target1 = "vo2_max"
target2 = "progress_score"

# Extract Features and Targets
X = df[features]
y1 = df[target1]  # Target for VO2_max
y2 = df[target2]  # Target for progress_score

# Split dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.2, random_state=42)

# Scale the data (important for neural networks)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Display shapes of the datasets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y1_train shape (VO2_max):", y1_train.shape)
print("y2_train shape (progress_score):", y2_train.shape)

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor

# Initialize Models
print("\n[Creating Models]")

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
print("‚úÖ Random Forest Model Created.")

gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
print("‚úÖ Gradient Boosting Model Created.")

mlp_model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)
print("‚úÖ Neural Network Model Created.")

# Training the models
print("\n[Training Models]")

print("‚è≥ Training Random Forest Regressor...")
rf_model.fit(X_train, y1_train)
print("‚úÖ Random Forest Training Completed.\n")

print("‚è≥ Training Gradient Boosting Regressor...")
gb_model.fit(X_train, y1_train)
print("‚úÖ Gradient Boosting Training Completed.\n")

print("‚è≥ Training Neural Network Regressor...")
mlp_model.fit(X_train_scaled, y1_train)  # Neural Network requires scaled data
print("‚úÖ Neural Network Training Completed.\n")

import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score

# Predictions for VO2_max
print("\n[Evaluating Models for VO2_max]")

rf_preds = rf_model.predict(X_test)
gb_preds = gb_model.predict(X_test)
mlp_preds = mlp_model.predict(X_test_scaled)  # Neural Network requires scaled data

# Compute MSE and R¬≤ scores
results = {
    "Model": ["Random Forest", "Gradient Boosting", "Neural Network"],
    "MSE": [
        mean_squared_error(y1_test, rf_preds),
        mean_squared_error(y1_test, gb_preds),
        mean_squared_error(y1_test, mlp_preds)
    ],
    "R¬≤ Score": [
        r2_score(y1_test, rf_preds),
        r2_score(y1_test, gb_preds),
        r2_score(y1_test, mlp_preds)
    ]
}

# Create DataFrame for better visualization
results_df = pd.DataFrame(results)

# Display results in a formatted table
print("\nüìä Model Evaluation Results for VO2_max:\n")
print(results_df)

# Find the best model based on highest R¬≤ Score
best_model = results_df.loc[results_df["R¬≤ Score"].idxmax(), "Model"]
best_r2 = results_df["R¬≤ Score"].max()

print(f"\nüèÜ The best model for predicting VO2_max is: **{best_model}** with R¬≤ Score: {best_r2:.4f}\n")

# Initialize Models for progress_score
print("\n[Creating Models for progress_score]")

rf_model_y2 = RandomForestRegressor(n_estimators=100, random_state=42)
print("‚úÖ Random Forest Model Created.")

gb_model_y2 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
print("‚úÖ Gradient Boosting Model Created.")

mlp_model_y2 = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)
print("‚úÖ Neural Network Model Created.")

# Training the models for progress_score
print("\n[Training Models for progress_score]")

print("‚è≥ Training Random Forest Regressor...")
rf_model_y2.fit(X_train, y2_train)
print("‚úÖ Random Forest Training Completed.\n")

print("‚è≥ Training Gradient Boosting Regressor...")
gb_model_y2.fit(X_train, y2_train)
print("‚úÖ Gradient Boosting Training Completed.\n")

print("‚è≥ Training Neural Network Regressor...")
mlp_model_y2.fit(X_train_scaled, y2_train)  # Neural Network requires scaled data
print("‚úÖ Neural Network Training Completed.\n")

# Predictions for progress_score
print("\n[Evaluating Models for progress_score]")

rf_preds_y2 = rf_model_y2.predict(X_test)
gb_preds_y2 = gb_model_y2.predict(X_test)
mlp_preds_y2 = mlp_model_y2.predict(X_test_scaled)  # Neural Network requires scaled data

# Compute MSE and R¬≤ scores
results_y2 = {
    "Model": ["Random Forest", "Gradient Boosting", "Neural Network"],
    "MSE": [
        mean_squared_error(y2_test, rf_preds_y2),
        mean_squared_error(y2_test, gb_preds_y2),
        mean_squared_error(y2_test, mlp_preds_y2)
    ],
    "R¬≤ Score": [
        r2_score(y2_test, rf_preds_y2),
        r2_score(y2_test, gb_preds_y2),
        r2_score(y2_test, mlp_preds_y2)
    ]
}

# Create DataFrame for better visualization
results_df_y2 = pd.DataFrame(results_y2)

# Display results in a formatted table
print("\nüìä Model Evaluation Results for progress_score:\n")
print(results_df_y2)

# Find the best model based on highest R¬≤ Score
best_model_y2 = results_df_y2.loc[results_df_y2["R¬≤ Score"].idxmax(), "Model"]
best_r2_y2 = results_df_y2["R¬≤ Score"].max()

print(f"\nüèÜ The best model for predicting progress_score is: **{best_model_y2}** with R¬≤ Score: {best_r2_y2:.4f}\n")

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")

# Bar Plot for MSE
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df_y2["Model"], y=results_df_y2["MSE"], palette="Blues_r")
plt.title("üìâ Model Comparison - Mean Squared Error (MSE)")
plt.ylabel("MSE (Lower is Better)")
plt.xlabel("Models")
plt.show()

# Bar Plot for R¬≤ Score
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df_y2["Model"], y=results_df_y2["R¬≤ Score"], palette="Greens_r")
plt.title("üìà Model Comparison - R¬≤ Score")
plt.ylabel("R¬≤ Score (Higher is Better)")
plt.xlabel("Models")
plt.show()

# Scatter Plot: Actual vs. Predicted (Best Model)
best_preds_y2 = None

if best_model_y2 == "Random Forest":
    best_preds_y2 = rf_preds_y2
elif best_model_y2 == "Gradient Boosting":
    best_preds_y2 = gb_preds_y2
elif best_model_y2 == "Neural Network":
    best_preds_y2 = mlp_preds_y2

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y2_test, y=best_preds_y2, alpha=0.5)
plt.plot([min(y2_test), max(y2_test)], [min(y2_test), max(y2_test)], color="red", linestyle="dashed")  # 45-degree line
plt.xlabel("Actual progress_score")
plt.ylabel("Predicted progress_score")
plt.title(f"üéØ Actual vs. Predicted ({best_model_y2})")
plt.show()

# Bar Plot for MSE (VO2_max)
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df["Model"], y=results_df["MSE"], palette="Blues_r")
plt.title("üìâ Model Comparison - Mean Squared Error (MSE) for VO2_max")
plt.ylabel("MSE (Lower is Better)")
plt.xlabel("Models")
plt.show()

# Bar Plot for R¬≤ Score (VO2_max)
plt.figure(figsize=(10, 5))
sns.barplot(x=results_df["Model"], y=results_df["R¬≤ Score"], palette="Greens_r")
plt.title("üìà Model Comparison - R¬≤ Score for VO2_max")
plt.ylabel("R¬≤ Score (Higher is Better)")
plt.xlabel("Models")
plt.show()

# Scatter Plot: Actual vs. Predicted (Best Model for VO2_max)
best_preds_y1 = None

if best_model == "Random Forest":
    best_preds_y1 = rf_preds
elif best_model == "Gradient Boosting":
    best_preds_y1 = gb_preds
elif best_model == "Neural Network":
    best_preds_y1 = mlp_preds

plt.figure(figsize=(8, 6))
sns.scatterplot(x=y1_test, y=best_preds_y1, alpha=0.5)
plt.plot([min(y1_test), max(y1_test)], [min(y1_test), max(y1_test)], color="red", linestyle="dashed")  # 45-degree line
plt.xlabel("Actual VO2_max")
plt.ylabel("Predicted VO2_max")
plt.title(f"üéØ Actual vs. Predicted ({best_model}) for VO2_max")
plt.show()

import numpy as np

# Creating a sample test input (Modify values based on dataset ranges)
sample_input = np.array([[30, 1, 22.5, 8000, 85, 250, 2, 3, 70]])  # Example input
sample_input_scaled = scaler.transform(sample_input)  # Scale for MLP

# Predict VO2_max using the best model
print("\nüîç Testing Best VO2_max Model:")
if best_model == "Random Forest":
    vo2_prediction = rf_model.predict(sample_input)[0]
elif best_model == "Gradient Boosting":
    vo2_prediction = gb_model.predict(sample_input)[0]
elif best_model == "Neural Network":
    vo2_prediction = mlp_model.predict(sample_input_scaled)[0]

print(f"‚úÖ Predicted VO2_max: {vo2_prediction:.2f}")

# Predict progress_score using the best model
print("\nüîç Testing Best progress_score Model:")
if best_model_y2 == "Random Forest":
    progress_prediction = rf_model_y2.predict(sample_input)[0]
elif best_model_y2 == "Gradient Boosting":
    progress_prediction = gb_model_y2.predict(sample_input)[0]
elif best_model_y2 == "Neural Network":
    progress_prediction = mlp_model_y2.predict(sample_input_scaled)[0]

print(f"‚úÖ Predicted progress_score: {progress_prediction:.2f}")

import joblib

# Save best VO2_max model
if best_model == "Random Forest":
    joblib.dump(rf_model, "best_vo2_model.pkl")
elif best_model == "Gradient Boosting":
    joblib.dump(gb_model, "best_vo2_model.pkl")
elif best_model == "Neural Network":
    joblib.dump(mlp_model, "best_vo2_model.pkl")

print(f"‚úÖ Best VO2_max model ({best_model}) saved as 'best_vo2_model.pkl'.")

# Save best progress_score model
if best_model_y2 == "Random Forest":
    joblib.dump(rf_model_y2, "best_progress_model.pkl")
elif best_model_y2 == "Gradient Boosting":
    joblib.dump(gb_model_y2, "best_progress_model.pkl")
elif best_model_y2 == "Neural Network":
    joblib.dump(mlp_model_y2, "best_progress_model.pkl")

print(f"‚úÖ Best progress_score model ({best_model_y2}) saved as 'best_progress_model.pkl'.")

# Load the saved models
loaded_vo2_model = joblib.load("best_vo2_model.pkl")
loaded_progress_model = joblib.load("best_progress_model.pkl")

print("\n‚úÖ Models loaded successfully.")

# Define a new sample input (Modify values as needed)
new_sample = np.array([[28, 0, 24.0, 7000, 78, 220, 1, 4, 80]])  # Example input
new_sample_scaled = scaler.transform(new_sample)  # Scale for Neural Network

# Predict using the loaded VO2_max model
print("\nüîç Predicting VO2_max with the loaded model:")
if best_model == "Random Forest":
    new_vo2_pred = loaded_vo2_model.predict(new_sample)[0]
elif best_model == "Gradient Boosting":
    new_vo2_pred = loaded_vo2_model.predict(new_sample)[0]
elif best_model == "Neural Network":
    new_vo2_pred = loaded_vo2_model.predict(new_sample_scaled)[0]

print(f"‚úÖ Predicted VO2_max: {new_vo2_pred:.2f}")

# Predict using the loaded progress_score model
print("\nüîç Predicting progress_score with the loaded model:")
if best_model_y2 == "Random Forest":
    new_progress_pred = loaded_progress_model.predict(new_sample)[0]
elif best_model_y2 == "Gradient Boosting":
    new_progress_pred = loaded_progress_model.predict(new_sample)[0]
elif best_model_y2 == "Neural Network":
    new_progress_pred = loaded_progress_model.predict(new_sample_scaled)[0]

print(f"‚úÖ Predicted progress_score: {new_progress_pred:.2f}")